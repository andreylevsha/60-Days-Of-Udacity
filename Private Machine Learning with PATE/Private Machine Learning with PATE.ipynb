{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up Enviroment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import KFold\n",
    "import utils, aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data`b\n",
    "teacher_data = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "student_data = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the public data to train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train = None\n",
    "student_test = None\n",
    "for i, (train_index, test_index) in enumerate(kf.split(student_data)):\n",
    "    student_train = torch.utils.data.Subset(student_data, train_index)\n",
    "    student_test = torch.utils.data.Subset(student_data, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_teachers = 250\n",
    "num_labels = 10\n",
    "num_examples = 2048\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Teachers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "classifiers = [None for _ in range(num_teachers)]\n",
    "for i in range(num_teachers):\n",
    "    classifiers[i] = Teacher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training teachers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=num_teachers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t_i, (_, data_index) in enumerate(kf.split(teacher_data)):\n",
    "    print (\"Training model: {}\".format(t_i))\n",
    "    classifier = classifiers[t_i]\n",
    "    \n",
    "    data = torch.utils.data.Subset(teacher_data, data_index)\n",
    "    \n",
    "    kf_train = KFold(n_splits=2)\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "        train = torch.utils.data.Subset(data, train_index)\n",
    "        test = torch.utils.data.Subset(data, test_index)\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "        testloader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "\n",
    "        # Only train the classifier parameters, feature parameters are frozen\n",
    "        optimizer = optim.Adam(classifier.parameters(), lr=0.003)\n",
    "\n",
    "        classifier.to(device);\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, labels in trainloader:\n",
    "                steps += 1\n",
    "                # Move input and label tensors to the default device\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logps = classifier.forward(inputs)\n",
    "                loss = criterion(logps, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if (epoch+1) % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                classifier.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, labels in testloader:\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "                        logps = classifier.forward(inputs)\n",
    "                        batch_loss = criterion(logps, labels)\n",
    "\n",
    "                        test_loss += batch_loss.item()\n",
    "\n",
    "                        # Calculate accuracy\n",
    "                        ps = torch.exp(logps)\n",
    "                        top_p, top_class = ps.topk(1, dim=1)\n",
    "                        equals = top_class == labels.view(*top_class.shape)\n",
    "                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                      f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                      f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
    "                      f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
    "                running_loss = 0\n",
    "                classifier.train()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving/Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = []\n",
    "\n",
    "for i in range(len(classifiers)):\n",
    "    all_points += [classifiers[i].state_dict()]\n",
    "\n",
    "checkpoint = {'classifiers_list': all_points,}\n",
    "torch.save(checkpoint, 'checkpoints/teachers_checkpoint.pth') ###############################################\tSave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [None for _ in range(num_teachers)]\n",
    "for i in range(num_teachers):\n",
    "    classifiers[i] = Teacher()\n",
    "\n",
    "checkpoint = torch.load('checkpoints/teachers_checkpoint.pth')\n",
    "classifiers_list = checkpoint['classifiers_list']\n",
    "for i in range(len(classifiers_list)):\n",
    "    classifiers[i].load_state_dict(classifiers_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PATE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_preds = np.zeros((num_teachers, num_examples)).astype(int)\n",
    "truths = np.zeros((num_examples)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_train_loader = torch.utils.data.DataLoader(student_train, batch_size=batch_size)\n",
    "student_test_loader = torch.utils.data.DataLoader(student_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get teachers' predictions for 2048 samples in student_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_teachers):\n",
    "    classifier = classifiers[i]\n",
    "    criterion = nn.NLLLoss()\n",
    "    classifier.to(device)\n",
    "    \n",
    "    test_loss = 0\n",
    "    accuracy = 0\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_l1 = 0\n",
    "        for inputs, labels in student_train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logps = classifier.forward(inputs)\n",
    "            batch_loss = criterion(logps, labels)\n",
    "\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = torch.exp(logps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            \n",
    "            teacher_preds[i,preds_l1:(preds_l1 + batch_size)] = top_class.cpu().numpy().squeeze(1).astype(int)\n",
    "            truths[preds_l1:(preds_l1 + batch_size)] = labels.cpu().numpy()\n",
    "            if (preds_l1 + batch_size) >= num_examples:\n",
    "                break\n",
    "            preds_l1 += batch_size\n",
    "            \n",
    "    classifier.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute most votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = utils.cal_max(teacher_preds, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Independent Epsilon: 751.0955105579642\n",
      "Data Dependent Epsilon: 0.7707510458439465\n"
     ]
    }
   ],
   "source": [
    "from syft.frameworks.torch.differential_privacy import pate\n",
    "\n",
    "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=teacher_preds, indices=indices, noise_eps=0.3, delta=1e-6, moments=140)\n",
    "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
    "print(\"Data Dependent Epsilon:\", data_dep_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Training student model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose privacy loss level same as noise_eps in the Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_loss_lv = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Report Noisy Max algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_class = utils.noisy_max(teacher_preds, privacy_loss_lv, num_labels)\n",
    "equals = top_class == truths\n",
    "accuracy = np.mean(equals)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "student_model = Student()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15.. Train loss: 0.000.. \n",
      "Epoch 2/15.. Train loss: 0.451.. \n",
      "Epoch 3/15.. Train loss: 0.137.. \n",
      "Epoch 4/15.. Train loss: 0.082.. \n",
      "Epoch 5/15.. Train loss: 0.075.. \n",
      "Epoch 6/15.. Train loss: 0.052.. \n",
      "Epoch 7/15.. Train loss: 0.036.. \n",
      "Epoch 8/15.. Train loss: 0.050.. \n",
      "Epoch 9/15.. Train loss: 0.051.. \n",
      "Epoch 10/15.. Train loss: 0.058.. \n",
      "Epoch 11/15.. Train loss: 0.038.. \n",
      "Epoch 12/15.. Train loss: 0.014.. \n",
      "Epoch 13/15.. Train loss: 0.013.. \n",
      "Epoch 14/15.. Train loss: 0.012.. \n",
      "Epoch 15/15.. Train loss: 0.016.. \n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(student_model.parameters(), lr=0.003)\n",
    "student_model.to(device);\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    preds_l1 = 0\n",
    "    for inputs, true_labels in student_train_loader:\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # Return Report Noisy Max outputs\n",
    "        labels = torch.tensor(utils.noisy_max(teacher_preds=teacher_preds[:,steps*batch_size:(steps+1)*batch_size]\n",
    "                                              , privacy_loss_lv=privacy_loss_lv, n_labels=num_labels)).long()\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        if steps % print_every != 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logps = student_model.forward(inputs)\n",
    "            loss = criterion(logps, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \")\n",
    "            running_loss = 0\n",
    "            \n",
    "        if (preds_l1 + batch_size) >= num_examples:\n",
    "            break\n",
    "        preds_l1 += batch_size\n",
    "            \n",
    "        steps += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.757.. Test accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "student_model.to(device);\n",
    "\n",
    "test_loss = 0\n",
    "accuracy = 0\n",
    "student_model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in student_test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        logps = student_model.forward(inputs)\n",
    "        batch_loss = criterion(logps, labels)\n",
    "\n",
    "        test_loss += batch_loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        ps = torch.exp(logps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "print(f\"Test loss: {test_loss/len(student_test_loader):.3f}.. \"\n",
    "      f\"Test accuracy: {accuracy/len(student_test_loader):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
